\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mnih2015human}
\citation{mnih2015human}
\citation{rlintroduction}
\citation{zhang2000neural}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{I}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Reinforced Learning}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II}Q-Learning}{1}{subsection.1.2}}
\newlabel{eq:qlearning}{{1}{1}{Q-Learning}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III}Deep Learning}{1}{subsection.1.3}}
\citation{mnih2015human}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV}OpenAI Gym}{2}{subsection.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methods}{2}{section.2}}
\newlabel{sec:methods}{{II}{2}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Deep Q-Networks}{2}{subsection.2.1}}
\newlabel{eq:msedqn}{{3}{2}{Deep Q-Networks}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II}Policy Gradient}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experiments}{2}{section.3}}
\newlabel{sec:experiments}{{III}{2}{Experiments}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I}Different Network architectures}{2}{subsection.3.1}}
\citation{mnih2015human}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Different network architectures used with DQN.\relax }}{3}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:nn_arch}{{1}{3}{Different network architectures used with DQN.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II}$\epsilon $-greedy strategies}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III}Experience replay strategies}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{3}{section.4}}
\newlabel{sec:results}{{IV}{3}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I}DQN with Cart Pole}{3}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Episode rewards for architecture with one hidden layer with 24 neurons. \relax }}{3}{figure.caption.2}}
\newlabel{fig:nn_24}{{1}{3}{Episode rewards for architecture with one hidden layer with 24 neurons. \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Episode rewards for architecture with two hidden layers with 24 neurons. \relax }}{4}{figure.caption.3}}
\newlabel{fig:nn_24_24}{{2}{4}{Episode rewards for architecture with two hidden layers with 24 neurons. \relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Episode rewards for architecture with two hidden layers from which the first had 24 neurons and the second 48. \relax }}{4}{figure.caption.4}}
\newlabel{fig:nn_24_48}{{3}{4}{Episode rewards for architecture with two hidden layers from which the first had 24 neurons and the second 48. \relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Episode rewards for $\epsilon $-greedy strategy that had $\epsilon $ value over 0.1 for 1000 episodes. Decay value was 0.01 for the epsilon strategy in the formula 5. \relax }}{4}{figure.caption.5}}
\newlabel{fig:e-greedy}{{4}{4}{Episode rewards for $\epsilon $-greedy strategy that had $\epsilon $ value over 0.1 for 1000 episodes. Decay value was 0.01 for the epsilon strategy in the formula 5. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Episode rewards when replay memory stored only the 1000 observations and had the batch size of 64. \relax }}{4}{figure.caption.6}}
\newlabel{fig:experience_replay}{{5}{4}{Episode rewards when replay memory stored only the 1000 observations and had the batch size of 64. \relax }{figure.caption.6}{}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{mnih2015human}{1}
\bibcite{rlintroduction}{2}
\bibcite{zhang2000neural}{3}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{5}{section.5}}
\newlabel{sec:discussion}{{V}{5}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Appendix A: Dataset insights}{6}{section.6}}
\newlabel{appendix-a}{{VI}{6}{Appendix A: Dataset insights}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Appendix B: Pipeline}{6}{section.7}}
\newlabel{appendix-b}{{VII}{6}{Appendix B: Pipeline}{section.7}{}}
